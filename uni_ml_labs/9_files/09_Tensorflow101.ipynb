{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Tensorflow 101\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow has become the world's most influential and important machine-learning library. As of December 2023, getting the GPU to work requires a little bit of extra work because of some open issues with the (plethora) of involved libraries:\n",
    "\n",
    "[Github Issue Page](https://github.com/tensorflow/tensorflow/issues/62075)\n",
    "\n",
    "Because we're dealing with a very large package with tons of dependencies, I recommend to create a new Anaconda environment for it. In Anaconda, environments are like collections of packages tailored for certain workloads. The default environment is called ``base`` and is automatically activated on startup. To create a new environment, open the Anaconda prompt and type \n",
    "\n",
    "```\n",
    "conda create -n tf python=3.10\n",
    "```\n",
    "\n",
    "which creates a new environment called ``tf`` with a specific Python interpreter version. To activate it, type \n",
    "\n",
    "```\n",
    "conda activate tf\n",
    "```\n",
    "\n",
    "while ``conda deactivate`` can be used to switch back to the base environment. A quick ``conda list`` reveals that ``tf`` only consists of a bare minimum of packages, so you need to install again whatever you need. First, make sure that ```pip``` is installed which will allow us to get ```Tensorflow``` from a different repository: \n",
    "\n",
    "```\n",
    "conda install pip\n",
    "```\n",
    "\n",
    "Verify that the ```pip``` commands points to the local Python installation, not the system Python:\n",
    "\n",
    "```\n",
    "which pip\n",
    "```\n",
    "\n",
    "The output should be a folder inside your Anaconda installation, typically your home directory. Now you're ready to get Tensorflow and the necessary CUDA libraries:\n",
    "\n",
    "```\n",
    "pip install tensorflow[and-cuda]==2.14\n",
    "```\n",
    "\n",
    "Finish your setup by adding two more packages:\n",
    "\n",
    "```pip install ipykernel matplotlib```\n",
    "\n",
    "Once all packages are installed, we are ready to import Tensorflow into our notebook as well as the auxilary library ``keras``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow comes with a broad collection of ready-to-use datasets that can be downloaded from the internet. One of these is the MNIST fashion dataset (see problem sheet 8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashionMnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(trainImages, trainLabels), (testImages, testLabels) = fashionMnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the (excellent) Tensorflow online tutorial (upon which this introduction is based on) for an overview of all datasets available:\n",
    "\n",
    "https://www.tensorflow.org/tutorials?hl=en\n",
    "\n",
    "Typically, the datasets are provided as Numpy arrays. Let's have a look at the arrays that have been downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainImages.shape, testImages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a total of 60000 training images and 10000 test images that fall into one of ten categories of clothing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label | Description\n",
    "--- | ---\n",
    "0 | T-shirt/top\n",
    "1 | Trouser\n",
    "2 | Pullover\n",
    "3 | Dress\n",
    "4 | Coat\n",
    "5 | Sandal\n",
    "6 | Shirt\n",
    "7 | Sneaker\n",
    "8 | Bag\n",
    "9 | Ankle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a sample of individual clothing item images - one for each kind - to get an idea of how these look like. For that, we will create a dictionary followed by a list of item descriptors which will serve as item keys. Within the loop, we go through the list of test data labels, get the associated descriptor and store it as variable ``item``. Then, we perform a check if this item is already part of the dictionary. If that's not the case, the image of the item is stored in the sample dictionary using the key ``item``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = {}\n",
    "descriptors = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Boot\"]\n",
    "nTrain = len(trainLabels)\n",
    "nTest = len(testLabels)\n",
    "\n",
    "for i in range(nTest):\n",
    "    item = descriptors[ testLabels[i] ]\n",
    "    if item not in sample:\n",
    "        sample[item] = testImages[i,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's subplot time again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, frames = plt.subplots(1, 10, sharex=True, sharey=True, figsize=(15,10))\n",
    "\n",
    "nPlots = len(descriptors)\n",
    "\n",
    "for i in range(nPlots):\n",
    "    item = descriptors[i]\n",
    "    frames[i].imshow(sample[item], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As final preprocessing step, we plot a distribution of all pixel values within the test image dataset to make sure we are dealing with normalized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(testImages.flatten(),16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already mostly zeros - which is nice - but we are not yet confined to the interval $[0,1]$. To achieve that, we need to divide all values by 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImages = testImages/255\n",
    "trainImages = trainImages/255\n",
    "\n",
    "plt.hist(testImages.flatten(),16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build the network! In Tensorflow, the typical workflow looks like this:\n",
    "\n",
    "- Prepare your data\n",
    "- Set up the layers of your neural network, including activation functions\n",
    "- Compile the model with a given numerical optimizer, loss function and optimization metric\n",
    "\n",
    "Now that we already have the data prepared i.e. have it loaded into Numpy training and test arrays, we can stack up the layers of the model. The first layer of most typical networks is a flattening layer, just as in our custom neural network, to reduce the dimensionality of our data - mostly for numerical reasons.\n",
    "\n",
    "Then, we add a ``Dense`` layer with 128 nodes which means that all nodes of the previous layer will be fully connected to 128 new nodes, defining the hidden layer. As an activation function, we use the rectified linear unit function which yields similar results as the sigmoid function but is computationally much less expensive. Finally, we add another dense layer with ten nodes and a softmax activation, representing the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to compile the model. While several numerical optimization routines exist, the ``adam`` optimizer is a good general choice. It automatically adapts the learning rate so we don't need to specify it though there is a flag for that. ``sparse_categorical_crossentropy`` is the loss-function of choice whenever we want our network to determine to which category some input belongs. Finally, the optimization metric is set to accuracy, i.e. we want the optimizer to find the best-fit accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train a model, it's always a good idea to print a model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, we need to optimize more than 100,000 matrix weights using our test and training data which is carried out by calling the ``fit`` method and passing the training images and labels as arguments. In addition, we validate the model using our test dataset right during the training process and specify a total of 10 training epochs. Let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(trainImages, trainLabels, validation_data=(testImages, testLabels), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 10 training epochs, we arrive at a test data accuracy of 91% and a validation data accuracy of 88%. Not too bad for such a simple model! A manual model evaluation can be triggered using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoss, testAcc = model.evaluate(testImages, testLabels)\n",
    "\n",
    "print('Network performance:', testAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ``predict`` method, we can automatically feed batches of data into the network and get the output it produces for every image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testImages)\n",
    "print(predictions.shape)\n",
    "\n",
    "print(predictions[0,:])\n",
    "\n",
    "plt.imshow(testImages[0], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the network is almost 99% sure that test image 0 is an ankle boot. A handy tool to get a quick evaluation of the network performance is a confusion matrix which compares all predictions with all labels and not only tells us how many shirts have been correctly classified as shirts, but also how many shirts have been classified as boots or jackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLabels = np.array( [np.argmax( vec ) for vec in predictions] )\n",
    "res = tf.math.confusion_matrix(testLabels,predictedLabels)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Tensorflow documentation, the matrix reads as follows:\n",
    "\n",
    "*The matrix columns represent the prediction labels and the rows represent the real labels. The confusion matrix is always a 2-D array of shape [n, n], where n is the number of valid labels for a given classification task. Both prediction and labels must be 1-D arrays of the same shape in order for this function to work.*\n",
    "\n",
    "What about individual images? Well, Tensorflow is optimized for batches of data, so we need to apply a little workaround to feed in single images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 43\n",
    "img = testImages[i]\n",
    "\n",
    "plt.title(descriptors[testLabels[i]])\n",
    "plt.imshow(img, cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do is to artificially expand the dimensionality of our 2D image to 3D, i.e. add a fake dimension. Then we can feed this expanded image into the network and see the output it produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgExp = (np.expand_dims(img,0))\n",
    "prediction = model.predict(imgExp)\n",
    "\n",
    "print(prediction.shape)\n",
    "print(prediction[0,:])\n",
    "print(\"Maximum:\", descriptors[np.argmax(prediction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's indeed a sneaker! \n",
    "\n",
    "Once we've trained our model - which can take hours, days or even weeks depending on the number of fit parameters, the complexity of the network and the size of the training dataset - it's advisable to store the model by calling the ``save`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('fashionNet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single line of code is sufficient to restore the model and reuse it in, e.g., a new notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newModel = keras.models.load_model('fashionNet.h5')\n",
    "\n",
    "newModel.summary()\n",
    "\n",
    "testLoss, testAcc = newModel.evaluate(testImages, testLabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
