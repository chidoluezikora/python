{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4: Numpy and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python lists are easy to use and versatile. Once you've mastered the basic syntax, array stuff in other languages will quickly become annoying. \n",
    "\n",
    "However, the downside of this flexibility is poor performance as the data cannot be efficiently arranged in the memory, nor vectorized or parallelized in obvious ways to support modern SIMD instruction extensions such as AVX2. This is where ``Numpy`` shines: Its goal is not only to provide methods and functions to simplify various sorts of everyday numerical operations, but also to provide a new data type that trades back a little bit of flexibility for a huge performance plus. Instead of lists, numpy uses arrays with a fixed data type such as ``double`` or ``int``. Python lists can directly be converted into arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ints = np.array( [1,1,2,3,5] )\n",
    "floats = np.array( [1.,1.,2.,3.,5.] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing one-dimensional arrays is just like accessing list elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ints)\n",
    "print(ints[0])\n",
    "print(floats[-1])\n",
    "print(floats[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested lists can be converted into 2d arrays. Accessing elements in such higher-dimensional arrays works similar like accessing nested lists, but we only have one set of brackets with comma-separated indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.array( [[1,2,3],[4,5,6],[7,8,9]] )\n",
    "\n",
    "print(mat)\n",
    "print()\n",
    "print(mat.shape)\n",
    "print()\n",
    "print(mat[0,0])\n",
    "print(mat[0,1])\n",
    "print()\n",
    "print(mat[:,0])\n",
    "print(mat[0,:])\n",
    "print()\n",
    "print(mat[0:2,0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements can also be selected based on some condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = mat[mat%2==0]\n",
    "\n",
    "print( sub.shape )\n",
    "print( sub )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data type is chosen automatically but can also be set explicitly. To check the datatype, check the ``dtype`` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ints.dtype)\n",
    "print(floats.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an array of zeros with some given dimensionality, use the ``zeros`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr0 = np.zeros(5)\n",
    "arr1 = np.zeros([2,3], dtype=\"int\")\n",
    "\n",
    "print(arr0)\n",
    "print(arr0.dtype)\n",
    "print()\n",
    "print(arr1)\n",
    "print(arr1.shape)\n",
    "print(arr1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default datatype is ``float64`` aka ``double``. To pre-set an array with ones instead of zeros, use the ``ones`` method while ``empty`` returns an un-initialized array. ``fill`` allows you to initialize an array with given dimension and some default value.\n",
    "\n",
    "Naturally, ``Numpy`` has built-in functions to create linearly spaced elements. The two most common ones used are ``arange`` and ``linspace``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = np.arange(0,5,0.5) # Just like the range function, but also works with float spacing\n",
    "vals2 = np.linspace(0,5,11) # Generate 11 values between 0. and 5., including the borders\n",
    "\n",
    "print(vals1, len(vals1))\n",
    "print(vals2, len(vals2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where ``Numpy`` really shines is its support for all-array vector operations. For every mathematical function in Pythons ``math`` library there is a ``Numpy`` equivalent with the same name. As the ``Numpy`` functions also support normal datatypes such as integers, floats and lists, and generally give better performance thanks to the ```Intel Math Kernel Library (MKL)```, I recommend using the ``Numpy`` functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "x = np.arange(0.,5.,11)\n",
    "y = f(x) # The function f is applied to all values in the array x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have expected, ``Numpy`` arrays are compatible with ``Matplotlib``.\n",
    "\n",
    "Thanks to ``Numpy``, we can manipulate entire arrays in a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x**2 + y + 3. # Square every entry of x, add y element-wise and add 3 to every element\n",
    "z = np.sqrt(np.arange(0,30,1))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy provides a plethora of all-array functions that help to get a first impression of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum: {0:f}\".format(np.max(z)))\n",
    "print(\"Minimum: {0:f}\".format(np.min(z)))\n",
    "print(\"Sum: {0:f}\".format(np.sum(z)))\n",
    "print(\"Average: {0:f}\".format(np.mean(z)))\n",
    "print(\"Standard deviation: {0:f}\".format(np.std(z)))\n",
    "print(\"Median: {0:f}\".format(np.median(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Numpy`` also provides solutions for your everyday linear algebra problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecA = np.array( [2,-1,4] )\n",
    "vecB = np.array( [-1,-2,5] )\n",
    "\n",
    "matA = np.array( [[2,-1,4],[-1,-2,5],[4,-2,8]] )\n",
    "\n",
    "print(\"Cross-product of vectors A and B:\", np.cross(vecA,vecB))\n",
    "print(\"Matrix A times vector A: \\n\", np.dot(matA, vecA))\n",
    "print(\"Matrix A squared: \\n\", np.dot(matA, matA))\n",
    "print(\"Determinant of Matrix A:\", np.linalg.det(matA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Numpy`` also provides functions to get data into files and back into your RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(x,a):\n",
    "    return x**a\n",
    "\n",
    "x = np.arange(0,1,0.01)\n",
    "a = np.arange(0,3.5,0.5)\n",
    "\n",
    "lines = len(x)\n",
    "cols = len(a)\n",
    "data = np.zeros( [lines,cols] )\n",
    "print(data.shape)\n",
    "\n",
    "for i in range(lines):\n",
    "    for j in range(cols):\n",
    "        data[i,j] = poly(x[i],a[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExp = np.insert(data,0,x,axis=1) # Append x values as first column\n",
    "\n",
    "print(dataExp.shape)\n",
    "\n",
    "np.savetxt(\"data.txt\", dataExp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"data.txt\")\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,0] # Extract the first column\n",
    "\n",
    "sqt = data[:,2] # Extract the third column\n",
    "lin = data[:,3]\n",
    "quad = data[:,5]\n",
    "\n",
    "print(sqt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Numpy`` is just perfect when you're dealing with purely numerical data. But in the real-world, datasets regularly contain string-type arguments such as the date or some category which makes getting the data into your Python program somewhat more tricky (see ``np.genfromtxt``). In addition, working with ``Numpy`` indices feels more like C (or Fortran...) and is not entirely in line with *the zen of Python*, where we can work with such beautiful data structures as dictionaries that allow us to access data based on strings instead of indices. For instance, for a Python purist data in the popular CSV (comma-separated values) format seems to be predestined for reading into a dictionary where every column forms a list and can be accessed via the column header specified in the first line of the file: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CSV Header in Notepad++](csv_header.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are dealing with time information in the English date format, floats, integers and nulls indicating days with either no trading or simply missing information. Python lists are flexible enough to deal with either, a simple function that does the trick of reading a CSV file into a dictionary just takes a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadCSV(fname):\n",
    "    data = {}\n",
    "    with open(fname, \"r\") as file:\n",
    "        header = file.readline().rstrip().split(\",\")\n",
    "        for head in header:\n",
    "            data[head] = []\n",
    "        for line in file:\n",
    "            cols = line.rstrip().split(\",\")\n",
    "            for col in zip(header,cols):\n",
    "                try:\n",
    "                    data[col[0]].append(float(col[1]))\n",
    "                except:\n",
    "                    data[col[0]].append(col[1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting the correct data type is where things start to get a little bit annoying, so I have even included some runtime error-checking to automatically convert all kinds of numerical data into floats with strings being the fallback option in case that the conversion fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadCSV(\"Gold.csv\")\n",
    "\n",
    "print(\"Dictionary keys:\", data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing and inspecting the data dictionary is convenient and straight-forward, we can even do some basic statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start date:\", data[\"Date\"][0])\n",
    "print(\"End date:\", data[\"Date\"][-1])\n",
    "print(\"Number of entries:\", len(data[\"Date\"]))\n",
    "print(\"Missing entries\", data[\"Open\"].count(\"null\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced numerical analysis fails because we're mixing floats and strings even within one column of data, so we would need to clean up our data dictionary first. But even then, stuff as list comprehensions over several columns quickly gets tiresome and brings back stuff, that we wanted to avoid: Working with indices ... \n",
    "\n",
    "Thankfully, Python provides a library that combines the speed of ``numpy`` with the flexibility and readability of simple Python structures: ``Pandas`` which is short for *Python Data Analysis Library*. Everything revolves around dataframes which consist of series, roughly equivalent to tables and columns. A dataframe can be created from nested lists where every element of the main list represents one line of the table. Pandas takes the column headers as a list of strings and creates a nice table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "month = [\"Jan-2021\", \"Feb-2021\", \"Mar-2021\"]\n",
    "profit = [638, 436, 887]\n",
    "debt = [-3554, -3145, -2901]\n",
    "\n",
    "someData = list(zip(month,profit,debt))\n",
    "\n",
    "df = pd.DataFrame(someData,columns=[\"Month\",\"Profit\",\"Debt\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries can be directly converted into dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among other file types such as JSON, XLS, ODS or HDF5, CSV files can be directly read into a dateframe, followed by a call to the ``info`` method to get some basic information regarding the structure of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Gold.csv\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas automatically detected a total of 5231 dates with the generic datatype *object* in column 0, 5119 non-null values in columns 1 to 6 and set the datatype of these columns to float64. For numerical data, die ```describe``` method is a good start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataseries or columns can be selected using the column header string and dictionary syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subDf = df[[\"Open\",\"Low\",\"High\"]]\n",
    "\n",
    "print(subDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate just a subset of rows, use the ``iloc`` method to slice your dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subDf.iloc[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select data based on a criterion such as a numerical threashold or a substring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subDf[ subDf[\"High\"] > 1000 ])\n",
    "\n",
    "print(df[ df[\"Date\"].str.contains(\"2012\")  ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the ``keys()`` method provided by dictionaries, a short onliner is sufficient to create a list of all column names which comes handy when we need to iterate through a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do element-wise calculations with entire columns similar to ``numpy`` arrays. Here, we will calculate the daily fluctuation of the gold price relative to the open price to quantify the daily volatility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vola = (subDf[\"High\"]-subDf[\"Low\"])/subDf[\"Open\"]\n",
    "print(vola)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be a dataseries which we can inspect further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Median Fluctuation:\", vola.median())\n",
    "print(\"Max Fluctuation:\", vola.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataseries can be added to existing dataframes just like new dictionary entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame() # Create an empty Dataframe\n",
    "results[\"Volatility\"] = vola\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All basic statistical quantities such as ``mean``, ``median`` or ``std`` can be calculated for entire frames selected series or subsets of series based on some sort of filter. ``Matplotlib`` natively supports ``Pandas`` dataseries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.hist(vola, 100)\n",
    "plt.axvline(vola.mean(), lw = 4, ls = \"--\", color = \"red\") # Arithmetic mean\n",
    "plt.axvline(vola.median(), lw = 4, ls = \"--\", color = \"orange\") # Median\n",
    "plt.axvline(list(vola.mode()), lw = 4, ls = \"--\", color = \"yellow\") # Mode of the distribution (returns a dataseries)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way: If you need to compare multiple distributions of some quantities, e.g. the volatility of various cryptocurrencies, boxplots are a good start. They include key information such as the median (orange line), the range between the upper and lower quartiles that contain 50% of all values as box surrounding the orange line, 1.5 times the quartiles as *whiskers* to visualize the spread of most of the values plus outliers as individual dots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.boxplot(results[\"Volatility\"].dropna(), labels=[\"Gold\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
