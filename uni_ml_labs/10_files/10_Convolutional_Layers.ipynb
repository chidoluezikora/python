{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*IMPORTANT*\n",
    "\n",
    "If you're experiencing issues with this notebook, please update your ```cuDNN``` library:\n",
    "\n",
    "```\n",
    "conda activate tf\n",
    "pip install nvidia-cudnn-cu11==8.9.0.131\n",
    "```\n",
    "\n",
    "Once that's done, import the libraries that we are going to use - no surprises so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the networks we have considered so far, all pixels of the input image data have been directly connected to the neurons of the hidden layer. This is fine as long as we are dealing with low-resolution image data, but as soon as we get to higher-quality input data, computational costs rapidly explode. In addition, the network gets even more sensitive to slight variations in the position of features even. Two kinds of special network layers exist to deal with these issues: Convolutional layers and pooling layers. \n",
    "\n",
    "Convolutional layers put the C into CNNs and can be depicted as little windows that slide over the input image and apply a small filter kernel matrix (typically) that extracts details that are relevant for the classification. In Tensorflow, we only need to provide the size of the filter kernel matrix as well as the number of filters that shall be applied. The best set of filter matrix elements is learned during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Convolutional layers](cnn.jpeg \"Title\")\n",
    "Source: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``scipy`` library contains a ``convolve`` function that allows us to easily apply some generic filter kernels with various effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install scipy\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "# A nice selection of kernels: https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "identity = np.array([[0, 0, 0],[0, 1, 0], [0, 0, 0]])\n",
    "edge = np.array([[1, 0, -1],[0, 0, 0],[-1, 0, 1]])\n",
    "sharpen = np.array([[0, -1, 0],[-1, 5, -1],[0, -1, 0]])\n",
    "gaussian = np.array([[1, 2, 1],[2, 4, 2],[1, 2, 1]]) * 0.0625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be more generic than a cat image? Here, we do some extra steps to display the image in its full pixel size assuming a screen pixel density of 100 dpi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(image.open('cat.png').convert('L'))\n",
    "\n",
    "dpi = 100\n",
    "height, width = img.shape\n",
    "figsize = width / dpi, height / dpi\n",
    "\n",
    "fig = plt.figure(figsize=figsize)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's apply the Gaussian blur filter to the cat image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blurImg = convolve(img, gaussian)\n",
    "blurImg = convolve(blurImg, gaussian)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "plt.imshow(blurImg, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the small size of the filter kernel, the softening effect is quite subtle. Nevertheless, the cat's whiskers and fur are clearly even softer than before. The ``sharpen`` kernel, on the other hand, results in the opposite effect: contrasts and edges are getting more prominent while smooth areas remain unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=figsize)\n",
    "plt.imshow(convolve(img, sharpen), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to neural networks: While we just applied a given filter kernel to a given image, a convolutional layer within a CNN does not have fixed kernel matrix elements. Rather, the network is trained to find which combination of elements works best for the dataset for feature extraction. While this may sound almost too good to be true, there is a hefty price that we are paying. Namely, the number of neural network parameters increases dramatically as does the time for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, after a convolutional layer, one typically applies a pooling layer (MaxPooling or AveragePooling) to reduce the complexity of the network by combining several nodes and only considering either the average or the maximum of their outputs, resulting in a dramatic reduction of data. We will apply these two new layers to a new dataset. That is, a collection of galaxy images that we will use to train a network to distinguish between three basic shapes: Elliptical, spiral and irregular galaxies.\n",
    "\n",
    "The data itself is a pre-selected subset of the EFIGI survey dataset:\n",
    "\n",
    "https://www.astromatic.net/projects/efigi\n",
    "\n",
    "First off, we need to get the images into the notebook. All relevant information is stored in the ``efigi.dat`` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"galaxies/efigi.dat\",\"r\")\n",
    "\n",
    "names = []\n",
    "types = []\n",
    "\n",
    "for line in data:\n",
    "    fields = line.split(\" \")\n",
    "    names.append( fields[0] )\n",
    "    types.append( fields[1] )\n",
    "    \n",
    "nData = len(names)\n",
    "imgSize = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elliptical galaxies belong to class 0, spirals to class 1 and irregulars to class 2. Now we can create the data arrays and iterate through the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxies = np.zeros((nData,imgSize,imgSize,3))\n",
    "labels = np.zeros(nData, dtype='int')\n",
    "\n",
    "for i in range(nData):\n",
    "    filename = \"galaxies/png/\"+str(names[i])+\".png\"\n",
    "    img = image.open(filename)\n",
    "\n",
    "    imgResized = img.resize(size=(imgSize,imgSize))\n",
    "    imgArr = np.array(imgResized)\n",
    "\n",
    "    imgArr = imgArr/255.\n",
    "    \n",
    "    galaxies[i,:,:,:] = imgArr \n",
    "    labels[i] = types[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need to flip the input data range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(galaxies.flatten(), 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the full dataset into a training and a test set. Here, index 955 marks the transition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 955\n",
    "\n",
    "trainGalaxies = galaxies[:split,:,:,:]\n",
    "trainLabels = labels[:split]\n",
    "\n",
    "testGalaxies = galaxies[split+1:nData-1,:,:,:]\n",
    "testLabels = labels[split+1:nData-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to go through all this again, we store the data as ``*.npz`` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"galaxies/trainGalaxies.npy\", trainGalaxies)\n",
    "np.save(\"galaxies/trainLabels.npy\", trainLabels)\n",
    "\n",
    "np.save(\"galaxies/testGalaxies.npy\", testGalaxies)\n",
    "np.save(\"galaxies/testLabels.npy\", testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to configure the network. The configuration we are using is based on a conference proceeding by an astrophysics group from Egypt (Khalifa et al 2017, https://arxiv.org/abs/1709.02245). We start with a feature extraction layer that uses a convolutional layer with 96 filters (8x8 filter kernel each) and a pooling layer that bins every 3x3 pixels to a single output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galNet = keras.Sequential([\n",
    "    keras.layers.Conv2D(96, (6, 6), activation='relu', input_shape=(imgSize,imgSize,3)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the structure of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galNet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to compile the model and train it. After every training epoch, the network performance is evalulated and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galNet.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = galNet.fit(trainGalaxies, trainLabels, validation_data=(testGalaxies, testLabels), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history object contains a history attribut that stores some of the training metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(history))\n",
    "print(type(history.history))\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the training process by looking at a plot of the evolution of the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Test set performance\")\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.savefig(\"galnet_performance.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, we're getting into the overfit regime. Are there ways to deal with that and train complex networks even further? More on that next week ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.open(\"galaxies/ngc1232b.jpg\")\n",
    "\n",
    "imgResized = img.resize(size=(imgSize,imgSize))\n",
    "imgArr = np.array(imgResized)\n",
    "\n",
    "imgArr = imgArr/255.\n",
    "\n",
    "plt.imshow(imgArr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgArrExp = np.expand_dims(imgArr,axis=0)\n",
    "print(imgArrExp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = galNet.predict(imgArrExp)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
